# -*- coding: utf-8 -*-
"""Rent_Prediction_Master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zw1lZK-PxLixZQ2e_akJWGBb9DJhFoeg
"""

import numpy as np
import pandas as pd

"""### Example show cases for data Reading:"""

df1 = pd.read_csv('./data/datasets.csv')

"""# Data Cleaning Master

"""

df1.head()

df1.shape

df1.info()

"""### Remove 'Rs', '$' and correct in the right format:"""

df1.replace({'hoa' : '[A-Z a-z $ ,]', 'rent amount' :'[A-Z a-z $ ,]' , 'property tax' : '[A-Z a-z $ ,]' , 
                         'fire insurance': '[A-Z a-z $ ,]', 'total': '[A-Z a-z $ ,]'} , '' , regex = True, inplace=True)

df1.head()

columns = df1.columns[9:]

columns

df1[columns].astype(str).astype(float)

for column in columns:
    for i in range(len(df1)):
        print(df1[column][i],'flag')
        df1[column][i] = float(df1[column][i])
        print(df1[column][i])

df1[columns] = df1[columns].replace('' , value = np.nan)
df1.head()

df1[columns] = df1[columns].astype(str).astype(float)

df1.info()

df1.head()

df1['floor'].unique()

df1['floor'].replace('-' , value = np.nan)

df1.head()

df1['floor'].replace('-' , value = np.NaN,inplace=True)

df1.head()

df1.info()

df1.isna().sum()/len(df1)

df1['floor'].unique()

df1['floor'].value_counts()

df1['floor'].value_counts().plot(kind='bar', figsize=(20,20))

"""<h3 id="deal_missing_values">Deal with missing data</h3>
<ol>
    <li>drop data<br>
        a. drop the whole row<br>
        b. drop the whole column
    </li>
    <li>replace data<br>
        a. replace it by mean/median<br>
        b. replace it by frequency<br>
        c. replace it based on other functions<br>
        d. replace with zero
    </li>
</ol>

Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely.
We have some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method to many different columns:

<b>Replace by mean/median:</b>
<ul>
    <li>"property tax ": 0.44% missing data, replace them with mean if no outlier else replace it with median.</li>
    <li>"hoa": 3.3% missing data, ~*replace them with mean if no outliers else with median*~ .</li>
</ul>

<b>Replace by frequency:</b>
<ul>
    <li>When should we apply it?.". 
        <ul>
            <li>let's say 80-90% of the floors blong to a particular category. It gives more sense replacing the most frequent number of floors with the missing ones, right?</li>
            <li>can we apply it to the missing value in the 'floor' columns? Let's discuss about it.</li>
        </ul>
    </li>
</ul>

<b>Drop the whole row:</b>
<ul>
    <li>when should we choose it over other ones?
        <ul>
            <li>1. price is what we want to predict. Any data entry without price data cannot be used for prediction; therefore it makes more sense droping the rows which belongs to the dependent variables.</li>
            <li>2. when you get a really huge dataset and the number of enteries corresponding to the missing values are comperatively negligible. But what is huge? It depends, we usually see it in percentage, e.g., less than 0.00001 of the missing value can be ignored. <b>Beaware of when you are dealing with fraud/spam detection problems.</li>
        </ul>
    </li>
</ul>

### "hoa"
"""

med_norm_loss_hoa = df1["hoa"].astype("float").median(axis=0)
avg_norm_loss_hoa = df1["hoa"].astype("float").mean(axis=0)
print("median_norm_loss:", med_norm_loss_hoa,'mean_norm_loss:',avg_norm_loss_hoa)

"""### "property-tax"""

med_norm_loss_pt = df1["property tax"].astype("float").median(axis=0)
avg_norm_loss_pt = df1["property tax"].astype("float").mean(axis=0)
print("median_norm_loss:", med_norm_loss_pt,'mean_norm_loss:',avg_norm_loss_pt)

"""### Replacement with median:"""

#replace nan of "hoa"
df1["hoa"].replace(np.nan, med_norm_loss_hoa, inplace=True)

#replace nan of "property tax"
df1["property tax"].replace(np.nan, med_norm_loss_pt, inplace=True)

#cross check
df1.isna().sum()

"""### Let's deal with "floor""""

100*df1['floor'].value_counts()/len(df1)

"""# What do you mean by the correlation?

"""

df1.corr()

"""### Do we need to encode categorical variables?"""

df1.columns

df1['animal'].unique()

df1['furniture'].unique()

cleanup_nums = {"animal":     {"acept": 1, "not acept": 0},
                "furniture": {"furnished": 1, "not furnished": 0}}

df1.replace(cleanup_nums, inplace=True)
df1.head()

df1.info()

df1['floor'] = df1['floor'].astype(int)

df1_floor = df1.dropna()

df1_floor.reset_index(inplace=True)

df1_floor.shape

df1_floor['floor'] = df1_floor['floor'].astype(int)

df1_floor.corr()

df1.shape

df1["floor"] = df1["floor"].fillna("0").astype(int)

df1['floor'].dtype

df1.info()

df1.corr()

"""### Conclusion on "floor"

<b>Does correlation tells anything important to us?</b>
<ul>
    <li>We don't see any strong correlation of "floor" with any other feature variables that ultimately shows good correlation with the price columns.</li>
    <li>Therefore, we conclude that no other features give good enough hint on what should be the right value we should choose to replace "nan' or now "O" in the floor column. </li>
</ul>


<b>Can we take mean/median?</b>
<ul>
    <li>It does not make any sense as we are dealing with categorical variable (floor). mean is uslually chosen when we are to replace nan in continous variable.</li>
    <li>Therefore, we reject mean. </li>
</ul>

<b>Can we drop the "floor" columns?</b>
<ul>
    <li>We would end up missing an important parameter that helps in predicting the price.</li>
    <li>Therefore, we reject this idea too. </li>
</ul>

<b>Can we drop all the rows that contain "nan" in the "floor"?</b>
<ul>
    <li>We would end up missing ~25% of our data which is significant to make prediction.</li>
    <li>Therefore, we reject this idea. </li>
</ul>

<b>Should we replace the "nan" with "0" or any other unique number?</b>
<ul>
    <li>It's the only option we am left with.</li>
    <li>Therefore, we accept it. </li>
</ul>
"""

df1.head()

df1.drop(df1.columns[df1.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

df1.info()

df1.to_csv('./data/clean_rent.csv', index=False)

"""
### Load the clean file:"""

df = pd.read_csv('./data/clean_rent.csv')

df.head()

per_ = pd.DataFrame(100*df['property tax']/df['rent amount'])



"""# EDA

### What is EDA?

Explore Data, simple!

Ask all set of question that you can and draw the plassuble insights.
"""

df.columns

"""#### Let me ask the following questions:

<li>How total price of the houses are distributed?</li>
<li>How rent amount is distributed?</li>
<li>Is there any relationship among different taxes(hoa, fire  insurance, property tax), rent amount and total rent?</li>
<li>How does city column influence the rent?</li>
<li>What are the floor demand of people?</li>
<li>Where does people tend to live? "City or not city"? Why?</li>
"""

import matplotlib.pyplot as plt
import seaborn as sns

"""## How total price of the houses are distributed?"""

df['total'].describe()

plt.figure(figsize =(10,10))
plt.subplot(2, 1, 1)
ax = sns.distplot(df['total'],kde =True)
#ax.set_xlim(0,1200000)
#ax.set_xticks(range(660,1200000,100000))
plt.subplot(2, 1, 2)
sns.boxplot(df['total'])

"""### Nature of the data:
<li>Right Skewed
<li>Having potential outliers after 100000

### To see the "total" more clearly, see the plot given below. It confirms our inference about the data
"""

plt.figure(figsize =(15,5))
ax = sns.rugplot(df['total'])
ax.set_xlim(0,1200000)
ax.set_xticks(range(499,1200000,100000))

"""### We've  found many outlliers, let's try to remove."""

q1 = df["total"].quantile(0.25)
q3 = df["total"].quantile(0.75)

IQR = q3 - q1
IF = q1 - (1.5 * IQR)
OF = q3 + (1.5 * IQR)

data = df[~((df["total"] < IF) | (df["total"] > OF))]
data.shape

print("Total number of dropped rows: ", df.shape[0]- data.shape[0])

"""### Let's visualise how an outlier influences the data:"""

plt.figure(figsize = (15,7))
sns.set(style = "whitegrid")
plt.subplot(1,2,1)
f = sns.barplot(x = "rooms", y = "total", data = df)
f.set_title("Before Removing Outliers")
f.set_xlabel("No. of Rooms")
f.set_ylabel("Total Cost")
plt.subplot(1,2,2)
f = sns.barplot(x = "rooms", y = "total", data = data)
f.set_title("After Removing Outliers")
f.set_xlabel("No. of Rooms")
f.set_ylabel("Total Cost")

"""# How rent amount is distributed

"""

data.head()

df['rent amount'].describe()

plt.figure(figsize =(10,10))
plt.subplot(2, 1, 1)
ax = sns.distplot(df['rent amount'],kde =True)
plt.subplot(2, 1, 2)
sns.boxplot(df['rent amount'])

plt.figure(figsize =(10,10))
plt.subplot(2, 1, 1)
ax = sns.distplot(data['rent amount'],kde =True)
plt.subplot(2, 1, 2)
sns.boxplot(data['rent amount'])

"""<li>Mostly the range is in between 450 -10000
<li>The Data is Right Skewed
<li>Potential outliers are there which are more than 10000
<li>The minimum rent is 420
<li>The maximum rent is 45000

#### what is the relationship among different taxes(hoa, fire insurance, property tax), rent amount and total rent
"""

df.head()

x = data['hoa'] + data['rent amount'] + data['property tax'] + data['fire insurance']

y = data['total']

plt.plot(x,y)

sns.heatmap(data[['hoa','rent amount','property tax','fire insurance','total']].corr(),annot =True)

"""### How does city column influence the rent?"""

g = sns.FacetGrid(data, col = 'city')
g = g.map(plt.hist,'rent amount')

"""<li>City has got larger demand.</li>

## Why is it so?
"""

sns.catplot(x = 'city', y='rooms', data =data, aspect=2, height =5 )

sns.countplot(data['animal'],hue = data['city'])

"""<li> Onle possible reason for why more people are opting to live in city is because they are getting more rooms and also they are allowed to keep pets.</li>

### What are the floor demand of people?
"""

data['floor'].value_counts().plot(kind='bar', figsize=(20,20))

"""#### From the graph it is quite evident that people demands lower floors w.r.t to the upper floors.

<h2 id="binning">Binning</h2>
<b>Why binning?</b>
<p>
    Binning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis.
</p>

<b>Example: </b>
<p>In our dataset, "floor" is a real valued variable ranging from 0 to 99 , it has 36 unique values. What if we only care about the price difference between floors with sky-floor, medium-floor and ground-floor (3 types)? Can we rearrange them into three ‘bins' to simplify analysis? </p>

<p>We will use the Pandas method 'cut' to segment the 'floor' column into 4 bins </p>
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(data["floor"])

# set x/y labels and plot title
plt.pyplot.xlabel("floor")
plt.pyplot.ylabel("count")
plt.pyplot.title("floor")

"""<p>We would like 3 bins of equal size bandwidth so we use numpy's <code>linspace(start_value, end_value, numbers_generated</code> function.</p>
<p>Since we want to include the minimum value of floor we want to set start_value=min(data["floor"]).</p>
<p>Since we want to include the maximum value of floor we want to set end_value=max(data["floor"]).</p>
<p>Since we are building 4 bins , there should be 5 dividers, so numbers_generated=5.</p>

We build a bin array, with a minimum value to a maximum value, with bandwidth calculated above. The bins will be values used to determine when one bin ends and another begins.
"""

#bins = np.linspace(min(data["floor"]), max(data["floor"]), 5)
bins = [0, 5,10,15,99]
bins

group_names = ['Low', 'low-Medium','upper-Medium', 'High']

group_names = ['Low', 'low-Medium','upper-Medium', 'High']
data['floor-binned'] = pd.cut(data['floor'], bins, labels=group_names, include_lowest=True )
data[['floor','floor-binned']].head(20)

data["floor-binned"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, data["floor-binned"].value_counts())

# set x/y labels and plot title
plt.pyplot.xlabel("floor")
plt.pyplot.ylabel("count")
plt.pyplot.title("floor bins")

"""We successfully narrow the intervals from 36 to 4!

Where does people tend to live?
"""

df.city.plot(kind='hist')

import matplotlib.pyplot as plt
cols = ['area', 'rooms', 'bathroom', 'parking spaces', 'floor','hoa', 'rent amount', 'property tax', 'fire insurance']
fig,ax = plt.subplots(nrows=3, ncols=3, figsize=(12,12))
for i,x in enumerate(cols):
    sns.scatterplot(x=x,y='total',data=data, ax=ax[i//3][i%3]);
plt.tight_layout()

sns.scatterplot(x='area',y='total',data=data)
#sns.plt.xlim(0, 1300)

data.area.max()

data1 = data[~((data["area"] == data.area.max()))]
data1.shape

sns.scatterplot(x='area',y='total',data=data1)

sns.boxplot(x='floor-binned', y='total',data=data1)

sns.boxplot(x='rooms', y='total',data=data1)

"""### some other possible questions are:

<li>Does the people are looking for more bathrooms? (Silly Right)
<li>Does furnishing impact in the rent?
<li>Whats the relationship between parking spaces and the total rent?
<li>What about Fire Insurance?
<li>What about the impact of property tax in the rent?
"""

data.head()

data.corr()

"""<b>P-value</b>: 
<p>What is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.</p>

By convention, when the
<ul>
    <li>p-value is $<$ 0.001: we say there is strong evidence that the correlation is significant.</li>
    <li>the p-value is $<$ 0.05: there is moderate evidence that the correlation is significant.</li>
    <li>the p-value is $<$ 0.1: there is weak evidence that the correlation is significant.</li>
    <li>the p-value is $>$ 0.1: there is no evidence that the correlation is significant.</li>
</ul>
"""

from scipy import stats

"""## area Vs total"""

pearson_coef, p_value = stats.pearsonr(data['area'], data['rent amount'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""## floor Vs total"""

pearson_coef, p_value = stats.pearsonr(data['floor'], data['rent amount'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>
<p>Since the p-value is $<$ 0.001, the correlation between area and floor and total are statistically significant, although the linear relationship isn't extremely strong (~0.36 and ~0.148 respectively)</p>

# ANOVA

<h3>ANOVA: Analysis of Variance</h3>
<p>The Analysis of Variance  (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:</p>

<p><b>F-test score</b>: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.</p>

<p><b>P-value</b>:  P-value tells how statistically significant is our calculated score value.</p>

<p>If our price variable is strongly correlated with the variable we are analyzing, expect ANOVA to return a sizeable F-test score and a small p-value.</p>

<p>Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.</p>

<p>Let's see if different types 'animal' impact  'total', we group the data.</p>
"""

grouped_test=data[['animal', 'rent amount']].groupby(['animal'])
grouped_test.head(2)

"""we can use the function 'f_oneway' in the module 'stats'  to obtain the <b>F-test score</b> and <b>P-value</b>."""

# ANOVA
f_val, p_val = stats.f_oneway(grouped_test.get_group(0)['rent amount'], grouped_test.get_group(1)['rent amount'])  
 
print( "ANOVA results: F=", f_val, ", P =", p_val)

"""This is a great result, with a large F test score showing a strong correlation and a P value of almost 0 implying almost certain statistical significance. """

grouped_test=data[['floor-binned', 'rent amount']].groupby(['floor-binned'])
grouped_test.head(2)

# ANOVA on floor-binned
f_val, p_val = stats.f_oneway(grouped_test.get_group('Low')['rent amount'], grouped_test.get_group('low-Medium')['rent amount'],grouped_test.get_group('upper-Medium')['rent amount'],grouped_test.get_group('High')['rent amount'])  
 
print( "ANOVA results: F=", f_val, ", P =", p_val)

"""# Final DATA"""

data.columns

data_f = data.drop(columns=['floor','property tax','fire insurance','total','hoa'])

cleanup_nums = {"floor-binned":     {"Low": 0, "low-Medium": 1,"upper-Medium": 2, "High": 3}}
data_f.replace(cleanup_nums, inplace=True)
data_f.head()

"""### Model predictions

Here we are going to set the models that we want use and the parameters we want to adopt. In this notebook I will use:<br>

Linear Regression<br>
Ridge Regression<br>
Decision Tree<br>
Random Forest<br>
Support Vector Regression (SVR) <br>
KNearestNeighbours (KNN)<br>
Lasso Regression<br>
GridSearch to find the best parameters on Lasso and Ridge<br>
"""

from sklearn.model_selection import train_test_split

#metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# models
from sklearn.preprocessing import PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

# Create a list to store all the results for later visualization
acc = []
# parameters are the alpha's that we will use to perform the GridSearch
parameters1= [{'alpha': [0.0001, 0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]
# on the regressors we define the models that we want use
regressors = {'Linear Regression': LinearRegression(),
              'Ridge Model': Ridge(alpha=0.1),
              'Decision Tree': DecisionTreeRegressor(),
              'Random Forest': RandomForestRegressor(random_state=1),
              'SVR': SVR(),
              'KNN': KNeighborsRegressor(),
              'Lasso': Lasso(),
              'GridSearchRidge': GridSearchCV(Ridge(), parameters1, cv=4),
              'GridSearchLasso': GridSearchCV(Lasso(), parameters1, cv=4)
             }

data_f.head()

cols = ['city', 'rooms', 'bathroom', 'parking spaces','animal','floor-binned','area',
        'furniture']
x = data_f[cols]
y = data_f['rent amount']
# Now we split into train and test
x_train, x_test, y_train, y_test = train_test_split(x,
                                                   y,
                                                   test_size = 0.3,
                                                   random_state = 0)

data_f.shape

# Perform a loop with each regressor to perform the model, predict the rent 
# and extract the metrics
for i in regressors:
    model = regressors.get(i)
    # here we create a condition because for grid we want to perform the model with the best estimator
    if i == 'GridSearchRidge' or i == 'GridSearchLasso':
        model.fit(x_train, y_train).best_estimator_ 
    model.fit(x_train, y_train)
    prediction = model.predict(x_test)
    print(i)
    print('MAE:', mean_absolute_error(y_test, prediction))
    print('RMSE:', np.sqrt(mean_squared_error(y_test, prediction)))
    print('R2:', r2_score(y_test, prediction))
    print('*' * 40)
    acc.append([i, mean_absolute_error(y_test, prediction), np.sqrt(mean_squared_error(y_test, prediction)), r2_score(y_test, prediction)])

# now let's follow the same loop and visualize the plot's for each regressor
j = 1
plt.figure(figsize=(20,10))
for i in regressors:
    model = regressors.get(i)
    model.fit(x_train, y_train)
    prediction = model.predict(x_test)
    plt.subplot(3, 3, j)
    plt.title(i)
    ax1 = sns.distplot(y_test,hist=False,kde =True,color ="r",label ="Actual Value")
    sns.distplot(prediction ,color ="b",hist = False,kde =True, label = "Predicted Value",ax =ax1).set_title(i)
    j+=1
plt.tight_layout(pad = 0.5)

"""Analysis of the results:"""

# lets sort our list of results and transform into a dataframe
acc.sort(key = lambda y:y[3], reverse=True)
acc = pd.DataFrame(data = acc, columns=['model', 'MAE', 'RMSE', 'R2'])

# now let's visualize it
acc.head(len(regressors))

"""RandomForest it's our best perfomer in all three metrics"""

# since RandomForest it's our best model, let's perform a rsquare test with differents
# degrees of polynomial transformation to see if we can improve it
rfr = RandomForestRegressor(random_state=1)
rfr.fit(x_train, y_train)
Rsqu_test = []

order = [1, 2, 3, 4]
for n in order:
    pr = PolynomialFeatures(degree=n)
    
    x_train_pr = pr.fit_transform(x_train)
    
    x_test_pr = pr.fit_transform(x_test)    
    
    rfr.fit(x_train_pr, y_train)
    
    Rsqu_test.append(rfr.score(x_test_pr, y_test))

plt.plot(order, Rsqu_test)
plt.xlabel('order')
plt.ylabel('R^2')
plt.title('R^2 Using Test Data')

data_f.shape





